{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twoim zadaniem jest wytrenowanie klasyfikatora binarnego na podzbiorze zbioru MNIST, w którym wyróżniamy klasy (cyfry 0 i 1 mają zostać wyłączone ze zbioru):\n",
    " - Liczby pierwsze (2,3,5,7)\n",
    " - Liczby złożone (4,6,8,9)\n",
    "\n",
    "Napisz wydajną implementację modelu **regresji logistycznej** trenowanego algorytmem ***SGD z momentum***. Cały proces trenowania musisz napisać samodzielnie, w języku Python, korzystając z biblioteki numpy. Na potrzeby zadania niedozwolone jest korzystanie z gotowych implementacji optimizerów i modeli oraz bibliotek do automatycznego różniczkowania funkcji (np. Tensorflow, pytorch, autograd). \n",
    "\n",
    "Dobierz hiperparametry tak, aby uzyskać jak najlepszy wynik na zbiorze walidacyjnym. \n",
    "Wyciągnij i zapisz wnioski z przeprowadzonych eksperymentów.\n",
    "\n",
    "Zbiór MNIST dostępny jest pod linkami: \n",
    "\n",
    "(zbiór treningowy):\n",
    " - http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "(zbiór walidacyjny):\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t: np.ndarray, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    Sigmoid (logistic) function:\n",
    "    calculates hypothesis value from given arrays of \n",
    "    coefficients thetas (t) and single observation (x)\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(t.T, x)\n",
    "    \n",
    "    #Avoid calculating e^(large numbers). In this case value of \n",
    "    #sigmoid function can be approximated to 0.\n",
    "    if dot_product < -10:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1/(1+np.exp(-dot_product)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(t: np.ndarray, X: np.ndarray, y: np.ndarray, \n",
    "        a, b, maxit, step, l):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent with momentum and L@ regularization\n",
    "    input:\n",
    "        - t: parameters theta (start vector)\n",
    "        - X: features set\n",
    "        - y: labels vector\n",
    "    parameters:\n",
    "        - defined in Model class\n",
    "    output:\n",
    "        - vector of thetas which optimized cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    #Number of features (theta params)\n",
    "    num_feat = t.size\n",
    "    #Number of observations\n",
    "    num_obs = len(X)\n",
    "    \n",
    "    #Initialize past time step with 0 value\n",
    "    v_prev = 0\n",
    "    \n",
    "    k=0\n",
    "    flag=True\n",
    "    while(flag):\n",
    "        \n",
    "        for row_id in range(0,num_obs,step):\n",
    "            \n",
    "            #Computing gradient\n",
    "            gradient = (sigmoid(t, X[row_id])-y[row_id])*X[row_id]\n",
    "            #Computing step\n",
    "            v = b*v_prev + a*gradient\n",
    "            #Actualizing thetas\n",
    "            t = t - v + (a*l/num_obs)*t\n",
    "            \n",
    "            v_prev = v\n",
    "            k+=1\n",
    "            if(k>maxit):\n",
    "                flag = False\n",
    "                break\n",
    "    \n",
    "    return(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Logistic regression with SGD-momentum solver\n",
    "    Parameters:\n",
    "        - a: learning parameter\n",
    "        - b: momentum parameter\n",
    "        - maxit: maximum number of iterations\n",
    "        - step: step while iterating over observations when calculating gradients\n",
    "        - l: L2 regularization strength \n",
    "    \"\"\"\n",
    "    #Theta parameters\n",
    "    t = None\n",
    "    \n",
    "    def __init__(self, a=1, b=0.9, maxit=10000, l=0, step=1):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.maxit = maxit\n",
    "        self.l = l\n",
    "        self.step = step\n",
    "    \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Trains model on given train set X and labels y\"\"\"\n",
    "        \n",
    "        #Initialize thetas to zeros vector\n",
    "        t_start = np.zeros(len(X[0]))\n",
    "        \n",
    "        #Run optimalization algorithm and assign output to t\n",
    "        self.t = sgd(t_start, X, y, self.a, self.b, self.maxit, self.step, self.l)\n",
    "        \n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes class prediction on given dataset\"\"\"\n",
    "        \n",
    "        probability = np.array([sigmoid(self.t, Xi) for Xi in X])\n",
    "        pred = np.where(probability>=0.5, 1, 0)\n",
    "        return(pred)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        # calculate accuracy\n",
    "        return(y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMNIST( prefix ):\n",
    "    \"\"\"Loads MNIST dataset and its labels from files\"\"\"\n",
    "    \n",
    "    intType = np.dtype('int32').newbyteorder('>')\n",
    "    nMetaDataBytes = 4 * intType.itemsize\n",
    "    \n",
    "    data = np.fromfile(\"./\" + prefix + '-images-idx3-ubyte', dtype = 'ubyte')\n",
    "    magicBytes, nImages, width, height = np.frombuffer(data[:nMetaDataBytes].tobytes(), intType)\n",
    "    data = data[nMetaDataBytes:].astype(dtype = 'float32').reshape([ nImages, width, height ])\n",
    "    data = data.reshape((data.shape[0], -1), order='F')\n",
    "    \n",
    "    labels = np.fromfile(\"./\" + prefix + '-labels-idx1-ubyte',dtype = 'ubyte')[2 * intType.itemsize:]\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = loadMNIST(\"train\")\n",
    "X_test, y_test = loadMNIST(\"t10k\")\n",
    "\n",
    "#Mapping orginal classes to binary as defined in task description\n",
    "y_train = np.isin(y_train, [2,3,5,7])*1\n",
    "y_test = np.isin(y_test, [2,3,5,7])*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short EDA\n",
    "For this porpouse I will use pandas and matplotlib packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = pd.DataFrame(X_train)\n",
    "temp_y = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance of target variable:\n",
    "Target classes are equal enough to assume that dataset is balanced,\n",
    "hence accuracy is right metric, techniques for imbalanced data are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    36225\n",
       "1    23775\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Values distribution\n",
    "Pixels values are extremely imbalanced. Most pixels' values are equal to 0 and the second significant group is set of values close to maximum (251-255). Because of that it is worth to consider if transforming images to black-and-white or normalization will improve results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    47040000.000000\n",
       "mean           33.495377\n",
       "std            73.746880\n",
       "min             0.000000\n",
       "25%             0.000000\n",
       "50%             0.000000\n",
       "75%             0.000000\n",
       "max           255.000000\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels_val = pd.DataFrame(temp_X.stack().value_counts())\n",
    "temp_X.stack().describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "plt.title(\"Pixels values histogram\")\n",
    "plt.bar(pixels_val.index, pixels_val[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant pixels\n",
    "Pixels (features) where all values are equal among all observations. Removing them from dataset seems to be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pixels with constant values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of pixels with constant values\")\n",
    "len(temp_X.std()[(temp_X.std() == 0)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and features engineering\n",
    "Basing on above brief EDA I will prepear dataset modified with feature engineering. In final model scoring I will compare results between this and orginal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Removing constant pixels\n",
    "def remove_constant_cols(train, test):\n",
    "    #Constant in train set\n",
    "    constant = np.all(train[1:] == train[:-1], axis=0)\n",
    "    #Remove same columns from train and test sets\n",
    "    train = train[:, ~constant]\n",
    "    test = test[:, ~constant]\n",
    "    return(train, test)\n",
    "\n",
    "X_train_fe, X_test_fe = remove_constant_cols(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Normalization\n",
    "X_train_fe = X_train/255\n",
    "X_test_fe = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "1. To find best hyperparameters I will use cross-validation method on training set.\n",
    "2. Best models will be scored on test set.\n",
    "3. Best results on orginal and modified dataset will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_tune_cv(model, k, params, X, y):\n",
    "    \"\"\"\n",
    "    Hyperparameters tuning with grid search c-v method\n",
    "    Parameters:\n",
    "        - params: matrix of parameters values to test\n",
    "        - k: number of folds\n",
    "    Output:\n",
    "        - matrix of results: [mean(acc), std(acc), params]\n",
    "    \"\"\"\n",
    "    a_set = params[0]\n",
    "    b_set = params[1]\n",
    "    maxit_set = params[2]\n",
    "    l_set = params[3]\n",
    "    step_set = params[4]\n",
    "    \n",
    "    #Creating cartesian product of parameters' values\n",
    "    params_set = [(a, b, maxit, l, step) for a in a_set \n",
    "                  for b in b_set \n",
    "                  for maxit in maxit_set \n",
    "                  for l in l_set\n",
    "                  for step in step_set]\n",
    "    \n",
    "    total = str(len(params_set))\n",
    "    \n",
    "    #Shuffling data in given X set and target, with the same permutation\n",
    "    id = np.random.permutation(len(X))\n",
    "    X = [X[i] for i in id]\n",
    "    y = [y[i] for i in id]\n",
    "        \n",
    "    num = int(np.floor(len(X)/k))\n",
    "    \n",
    "    #Creating lists of trainig and validation sets\n",
    "    validate_sets = [None]*k\n",
    "    train_sets = [None]*k\n",
    "    train_y = [None]*k\n",
    "    validate_y = [None]*k\n",
    "    \n",
    "    \n",
    "    for j in range(k-1):\n",
    "        X_validate = X[j*num : (j+1)*num]\n",
    "        X_train = np.delete(X, range(j*num, (j+1)*num), 0)\n",
    "        \n",
    "        y_validate = y[j*num : (j+1)*num]\n",
    "        y_train = np.delete(y, range(j*num, (j+1)*num))\n",
    "        \n",
    "        validate_sets[j] = X_validate\n",
    "        train_sets[j] = X_train\n",
    "        validate_y[j] = y_validate\n",
    "        train_y[j] = y_train\n",
    "    \n",
    "    validate_sets[k-1] = X[(k-1)*num:]\n",
    "    train_sets[k-1] = np.delete(X, range((k-1)*num, len(X)-1), 0)\n",
    "    validate_y[k-1] = y[(k-1)*num:]\n",
    "    train_y[k-1] = np.delete(y, range((k-1)*num, len(X)-1))\n",
    "    \n",
    "    #Now with ready k train and validetion sets computing mean and std of accuracy \n",
    "    #depends on different parameter values\n",
    "    results = []\n",
    "    \n",
    "    cur = 0\n",
    "    for i in params_set:\n",
    "        \n",
    "        acc = [None]*k\n",
    "        \n",
    "        for j in range(k):\n",
    "            \n",
    "            #Training model\n",
    "            log_reg = Model(a=i[0],b=i[1],maxit=i[2],l=i[3],step=i[4])\n",
    "            log_reg.fit(train_sets[j], train_y[j])\n",
    "            #Prediction and score on current validation set\n",
    "            pred = log_reg.predict(validate_sets[j])\n",
    "            score = log_reg.evaluate(validate_y[j], pred)\n",
    "            acc[j] = score\n",
    "            \n",
    "        r = [np.mean(acc), np.std(acc)]\n",
    "        r.extend(i)\n",
    "        results.append(r)\n",
    "        cur+=1\n",
    "        print(\"Done: \" + str(cur) + \" /\" + total)\n",
    "                \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of observations in single c-v train set\n",
    "k = 5\n",
    "num = (k-1)*len(X_train)/k\n",
    "\n",
    "#Parameters for grid search c-v\n",
    "par = [[0.01, 0.1, 0.9, 1, 1.1],\n",
    "      [0.01, 0.1, 0.9, 1, 1.1],\n",
    "      [0.1*num, 0.5*num, num, 2*num],\n",
    "      [0.001, 0.01, 0.1, 1, 10],\n",
    "      [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross_validation for orginal dataset\n",
    "result_org = param_tune_cv(Model(), 5, par, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 1 /500\n",
      "Done: 2 /500\n",
      "Done: 3 /500\n",
      "Done: 4 /500\n",
      "Done: 5 /500\n",
      "Done: 6 /500\n",
      "Done: 7 /500\n",
      "Done: 8 /500\n",
      "Done: 9 /500\n",
      "Done: 10 /500\n",
      "Done: 11 /500\n",
      "Done: 12 /500\n",
      "Done: 13 /500\n",
      "Done: 14 /500\n",
      "Done: 15 /500\n",
      "Done: 16 /500\n",
      "Done: 17 /500\n",
      "Done: 18 /500\n",
      "Done: 19 /500\n",
      "Done: 20 /500\n",
      "Done: 21 /500\n",
      "Done: 22 /500\n",
      "Done: 23 /500\n",
      "Done: 24 /500\n",
      "Done: 25 /500\n",
      "Done: 26 /500\n",
      "Done: 27 /500\n",
      "Done: 28 /500\n",
      "Done: 29 /500\n",
      "Done: 30 /500\n",
      "Done: 31 /500\n",
      "Done: 32 /500\n",
      "Done: 33 /500\n",
      "Done: 34 /500\n",
      "Done: 35 /500\n",
      "Done: 36 /500\n",
      "Done: 37 /500\n",
      "Done: 38 /500\n",
      "Done: 39 /500\n",
      "Done: 40 /500\n",
      "Done: 41 /500\n",
      "Done: 42 /500\n",
      "Done: 43 /500\n",
      "Done: 44 /500\n",
      "Done: 45 /500\n",
      "Done: 46 /500\n",
      "Done: 47 /500\n",
      "Done: 48 /500\n",
      "Done: 49 /500\n",
      "Done: 50 /500\n",
      "Done: 51 /500\n",
      "Done: 52 /500\n",
      "Done: 53 /500\n",
      "Done: 54 /500\n",
      "Done: 55 /500\n",
      "Done: 56 /500\n",
      "Done: 57 /500\n",
      "Done: 58 /500\n",
      "Done: 59 /500\n",
      "Done: 60 /500\n",
      "Done: 61 /500\n",
      "Done: 62 /500\n",
      "Done: 63 /500\n",
      "Done: 64 /500\n",
      "Done: 65 /500\n",
      "Done: 66 /500\n",
      "Done: 67 /500\n",
      "Done: 68 /500\n",
      "Done: 69 /500\n",
      "Done: 70 /500\n",
      "Done: 71 /500\n",
      "Done: 72 /500\n",
      "Done: 73 /500\n",
      "Done: 74 /500\n",
      "Done: 75 /500\n",
      "Done: 76 /500\n",
      "Done: 77 /500\n",
      "Done: 78 /500\n",
      "Done: 79 /500\n",
      "Done: 80 /500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in multiply\n",
      "/home/piotr/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 81 /500\n",
      "Done: 82 /500\n",
      "Done: 83 /500\n",
      "Done: 84 /500\n",
      "Done: 85 /500\n",
      "Done: 86 /500\n",
      "Done: 87 /500\n",
      "Done: 88 /500\n",
      "Done: 89 /500\n",
      "Done: 90 /500\n",
      "Done: 91 /500\n",
      "Done: 92 /500\n",
      "Done: 93 /500\n",
      "Done: 94 /500\n",
      "Done: 95 /500\n",
      "Done: 96 /500\n",
      "Done: 97 /500\n",
      "Done: 98 /500\n",
      "Done: 99 /500\n",
      "Done: 100 /500\n",
      "Done: 101 /500\n",
      "Done: 102 /500\n",
      "Done: 103 /500\n",
      "Done: 104 /500\n",
      "Done: 105 /500\n",
      "Done: 106 /500\n",
      "Done: 107 /500\n",
      "Done: 108 /500\n",
      "Done: 109 /500\n",
      "Done: 110 /500\n",
      "Done: 111 /500\n",
      "Done: 112 /500\n",
      "Done: 113 /500\n",
      "Done: 114 /500\n",
      "Done: 115 /500\n",
      "Done: 116 /500\n",
      "Done: 117 /500\n",
      "Done: 118 /500\n",
      "Done: 119 /500\n",
      "Done: 120 /500\n",
      "Done: 121 /500\n",
      "Done: 122 /500\n",
      "Done: 123 /500\n",
      "Done: 124 /500\n",
      "Done: 125 /500\n",
      "Done: 126 /500\n",
      "Done: 127 /500\n",
      "Done: 128 /500\n",
      "Done: 129 /500\n",
      "Done: 130 /500\n",
      "Done: 131 /500\n",
      "Done: 132 /500\n",
      "Done: 133 /500\n",
      "Done: 134 /500\n",
      "Done: 135 /500\n",
      "Done: 136 /500\n",
      "Done: 137 /500\n",
      "Done: 138 /500\n",
      "Done: 139 /500\n",
      "Done: 140 /500\n",
      "Done: 141 /500\n",
      "Done: 142 /500\n",
      "Done: 143 /500\n",
      "Done: 144 /500\n",
      "Done: 145 /500\n",
      "Done: 146 /500\n",
      "Done: 147 /500\n",
      "Done: 148 /500\n",
      "Done: 149 /500\n",
      "Done: 150 /500\n",
      "Done: 151 /500\n",
      "Done: 152 /500\n",
      "Done: 153 /500\n",
      "Done: 154 /500\n",
      "Done: 155 /500\n",
      "Done: 156 /500\n",
      "Done: 157 /500\n",
      "Done: 158 /500\n",
      "Done: 159 /500\n",
      "Done: 160 /500\n",
      "Done: 161 /500\n",
      "Done: 162 /500\n",
      "Done: 163 /500\n",
      "Done: 164 /500\n",
      "Done: 165 /500\n",
      "Done: 166 /500\n",
      "Done: 167 /500\n",
      "Done: 168 /500\n",
      "Done: 169 /500\n",
      "Done: 170 /500\n",
      "Done: 171 /500\n",
      "Done: 172 /500\n",
      "Done: 173 /500\n",
      "Done: 174 /500\n",
      "Done: 175 /500\n",
      "Done: 176 /500\n",
      "Done: 177 /500\n",
      "Done: 178 /500\n",
      "Done: 179 /500\n",
      "Done: 180 /500\n",
      "Done: 181 /500\n",
      "Done: 182 /500\n",
      "Done: 183 /500\n",
      "Done: 184 /500\n",
      "Done: 185 /500\n",
      "Done: 186 /500\n",
      "Done: 187 /500\n",
      "Done: 188 /500\n",
      "Done: 189 /500\n",
      "Done: 190 /500\n",
      "Done: 191 /500\n",
      "Done: 192 /500\n",
      "Done: 193 /500\n",
      "Done: 194 /500\n",
      "Done: 195 /500\n",
      "Done: 196 /500\n",
      "Done: 197 /500\n",
      "Done: 198 /500\n",
      "Done: 199 /500\n",
      "Done: 200 /500\n",
      "Done: 201 /500\n",
      "Done: 202 /500\n",
      "Done: 203 /500\n",
      "Done: 204 /500\n",
      "Done: 205 /500\n",
      "Done: 206 /500\n",
      "Done: 207 /500\n",
      "Done: 208 /500\n",
      "Done: 209 /500\n",
      "Done: 210 /500\n",
      "Done: 211 /500\n",
      "Done: 212 /500\n",
      "Done: 213 /500\n",
      "Done: 214 /500\n",
      "Done: 215 /500\n",
      "Done: 216 /500\n",
      "Done: 217 /500\n",
      "Done: 218 /500\n",
      "Done: 219 /500\n",
      "Done: 220 /500\n",
      "Done: 221 /500\n",
      "Done: 222 /500\n",
      "Done: 223 /500\n",
      "Done: 224 /500\n",
      "Done: 225 /500\n",
      "Done: 226 /500\n",
      "Done: 227 /500\n",
      "Done: 228 /500\n",
      "Done: 229 /500\n",
      "Done: 230 /500\n",
      "Done: 231 /500\n",
      "Done: 232 /500\n",
      "Done: 233 /500\n",
      "Done: 234 /500\n",
      "Done: 235 /500\n",
      "Done: 236 /500\n",
      "Done: 237 /500\n",
      "Done: 238 /500\n",
      "Done: 239 /500\n",
      "Done: 240 /500\n",
      "Done: 241 /500\n",
      "Done: 242 /500\n",
      "Done: 243 /500\n",
      "Done: 244 /500\n",
      "Done: 245 /500\n",
      "Done: 246 /500\n",
      "Done: 247 /500\n",
      "Done: 248 /500\n",
      "Done: 249 /500\n",
      "Done: 250 /500\n",
      "Done: 251 /500\n",
      "Done: 252 /500\n",
      "Done: 253 /500\n",
      "Done: 254 /500\n",
      "Done: 255 /500\n",
      "Done: 256 /500\n",
      "Done: 257 /500\n",
      "Done: 258 /500\n",
      "Done: 259 /500\n",
      "Done: 260 /500\n",
      "Done: 261 /500\n",
      "Done: 262 /500\n",
      "Done: 263 /500\n",
      "Done: 264 /500\n",
      "Done: 265 /500\n",
      "Done: 266 /500\n",
      "Done: 267 /500\n",
      "Done: 268 /500\n",
      "Done: 269 /500\n",
      "Done: 270 /500\n",
      "Done: 271 /500\n",
      "Done: 272 /500\n",
      "Done: 273 /500\n",
      "Done: 274 /500\n",
      "Done: 275 /500\n",
      "Done: 276 /500\n",
      "Done: 277 /500\n",
      "Done: 278 /500\n",
      "Done: 279 /500\n",
      "Done: 280 /500\n",
      "Done: 281 /500\n",
      "Done: 282 /500\n",
      "Done: 283 /500\n",
      "Done: 284 /500\n",
      "Done: 285 /500\n",
      "Done: 286 /500\n",
      "Done: 287 /500\n",
      "Done: 288 /500\n",
      "Done: 289 /500\n",
      "Done: 290 /500\n",
      "Done: 291 /500\n",
      "Done: 292 /500\n",
      "Done: 293 /500\n",
      "Done: 294 /500\n",
      "Done: 295 /500\n",
      "Done: 296 /500\n",
      "Done: 297 /500\n",
      "Done: 298 /500\n",
      "Done: 299 /500\n",
      "Done: 300 /500\n",
      "Done: 301 /500\n",
      "Done: 302 /500\n",
      "Done: 303 /500\n",
      "Done: 304 /500\n",
      "Done: 305 /500\n",
      "Done: 306 /500\n",
      "Done: 307 /500\n",
      "Done: 308 /500\n",
      "Done: 309 /500\n",
      "Done: 310 /500\n",
      "Done: 311 /500\n",
      "Done: 312 /500\n",
      "Done: 313 /500\n",
      "Done: 314 /500\n",
      "Done: 315 /500\n",
      "Done: 316 /500\n",
      "Done: 317 /500\n",
      "Done: 318 /500\n",
      "Done: 319 /500\n",
      "Done: 320 /500\n",
      "Done: 321 /500\n",
      "Done: 322 /500\n",
      "Done: 323 /500\n",
      "Done: 324 /500\n",
      "Done: 325 /500\n",
      "Done: 326 /500\n",
      "Done: 327 /500\n",
      "Done: 328 /500\n",
      "Done: 329 /500\n",
      "Done: 330 /500\n",
      "Done: 331 /500\n",
      "Done: 332 /500\n",
      "Done: 333 /500\n",
      "Done: 334 /500\n",
      "Done: 335 /500\n",
      "Done: 336 /500\n",
      "Done: 337 /500\n",
      "Done: 338 /500\n",
      "Done: 339 /500\n",
      "Done: 340 /500\n",
      "Done: 341 /500\n",
      "Done: 342 /500\n",
      "Done: 343 /500\n",
      "Done: 344 /500\n",
      "Done: 345 /500\n",
      "Done: 346 /500\n",
      "Done: 347 /500\n",
      "Done: 348 /500\n",
      "Done: 349 /500\n",
      "Done: 350 /500\n",
      "Done: 351 /500\n",
      "Done: 352 /500\n",
      "Done: 353 /500\n",
      "Done: 354 /500\n",
      "Done: 355 /500\n",
      "Done: 356 /500\n",
      "Done: 357 /500\n",
      "Done: 358 /500\n",
      "Done: 359 /500\n",
      "Done: 360 /500\n",
      "Done: 361 /500\n",
      "Done: 362 /500\n",
      "Done: 363 /500\n",
      "Done: 364 /500\n",
      "Done: 365 /500\n",
      "Done: 366 /500\n",
      "Done: 367 /500\n",
      "Done: 368 /500\n",
      "Done: 369 /500\n",
      "Done: 370 /500\n",
      "Done: 371 /500\n",
      "Done: 372 /500\n",
      "Done: 373 /500\n",
      "Done: 374 /500\n",
      "Done: 375 /500\n",
      "Done: 376 /500\n",
      "Done: 377 /500\n",
      "Done: 378 /500\n",
      "Done: 379 /500\n",
      "Done: 380 /500\n",
      "Done: 381 /500\n",
      "Done: 382 /500\n",
      "Done: 383 /500\n",
      "Done: 384 /500\n",
      "Done: 385 /500\n",
      "Done: 386 /500\n",
      "Done: 387 /500\n",
      "Done: 388 /500\n",
      "Done: 389 /500\n",
      "Done: 390 /500\n",
      "Done: 391 /500\n",
      "Done: 392 /500\n",
      "Done: 393 /500\n",
      "Done: 394 /500\n",
      "Done: 395 /500\n",
      "Done: 396 /500\n",
      "Done: 397 /500\n",
      "Done: 398 /500\n",
      "Done: 399 /500\n",
      "Done: 400 /500\n",
      "Done: 401 /500\n",
      "Done: 402 /500\n",
      "Done: 403 /500\n",
      "Done: 404 /500\n",
      "Done: 405 /500\n",
      "Done: 406 /500\n",
      "Done: 407 /500\n",
      "Done: 408 /500\n",
      "Done: 409 /500\n",
      "Done: 410 /500\n",
      "Done: 411 /500\n",
      "Done: 412 /500\n",
      "Done: 413 /500\n",
      "Done: 414 /500\n",
      "Done: 415 /500\n",
      "Done: 416 /500\n",
      "Done: 417 /500\n",
      "Done: 418 /500\n",
      "Done: 419 /500\n",
      "Done: 420 /500\n",
      "Done: 421 /500\n",
      "Done: 422 /500\n",
      "Done: 423 /500\n",
      "Done: 424 /500\n",
      "Done: 425 /500\n",
      "Done: 426 /500\n",
      "Done: 427 /500\n",
      "Done: 428 /500\n",
      "Done: 429 /500\n",
      "Done: 430 /500\n",
      "Done: 431 /500\n",
      "Done: 432 /500\n",
      "Done: 433 /500\n",
      "Done: 434 /500\n",
      "Done: 435 /500\n",
      "Done: 436 /500\n",
      "Done: 437 /500\n",
      "Done: 438 /500\n",
      "Done: 439 /500\n",
      "Done: 440 /500\n",
      "Done: 441 /500\n",
      "Done: 442 /500\n",
      "Done: 443 /500\n",
      "Done: 444 /500\n",
      "Done: 445 /500\n",
      "Done: 446 /500\n",
      "Done: 447 /500\n",
      "Done: 448 /500\n",
      "Done: 449 /500\n",
      "Done: 450 /500\n",
      "Done: 451 /500\n",
      "Done: 452 /500\n",
      "Done: 453 /500\n",
      "Done: 454 /500\n",
      "Done: 455 /500\n",
      "Done: 456 /500\n",
      "Done: 457 /500\n",
      "Done: 458 /500\n",
      "Done: 459 /500\n",
      "Done: 460 /500\n",
      "Done: 461 /500\n",
      "Done: 462 /500\n",
      "Done: 463 /500\n",
      "Done: 464 /500\n",
      "Done: 465 /500\n",
      "Done: 466 /500\n",
      "Done: 467 /500\n",
      "Done: 468 /500\n",
      "Done: 469 /500\n",
      "Done: 470 /500\n",
      "Done: 471 /500\n",
      "Done: 472 /500\n",
      "Done: 473 /500\n",
      "Done: 474 /500\n",
      "Done: 475 /500\n",
      "Done: 476 /500\n",
      "Done: 477 /500\n",
      "Done: 478 /500\n",
      "Done: 479 /500\n",
      "Done: 480 /500\n",
      "Done: 481 /500\n",
      "Done: 482 /500\n",
      "Done: 483 /500\n",
      "Done: 484 /500\n",
      "Done: 485 /500\n",
      "Done: 486 /500\n",
      "Done: 487 /500\n",
      "Done: 488 /500\n",
      "Done: 489 /500\n",
      "Done: 490 /500\n",
      "Done: 491 /500\n",
      "Done: 492 /500\n",
      "Done: 493 /500\n",
      "Done: 494 /500\n",
      "Done: 495 /500\n",
      "Done: 496 /500\n",
      "Done: 497 /500\n",
      "Done: 498 /500\n",
      "Done: 499 /500\n",
      "Done: 500 /500\n"
     ]
    }
   ],
   "source": [
    "#Cross_validation for dataset after feature engineering\n",
    "result_fe = param_tune_cv(Model(), 5, par, X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results for further work\n",
    "np.savetxt(\"orginal.csv\", result_orginal, fmt= '%.3e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"fe.csv\", result_fe, fmt= '%.3e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_org = pd.read_csv(\"orginal.csv\", header=None, sep=\" \")\n",
    "result_fe = pd.read_csv(\"fe.csv\", header=None, sep=\" \")\n",
    "result_org.columns = [\"mean_acc\",\"std_acc\",\"a\",\"b\",\"maxit\",\"l\",\"step\"]\n",
    "result_fe.columns = [\"mean_acc\",\"std_acc\",\"a\",\"b\",\"maxit\",\"l\",\"step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_org = result_org.sort_values(\"mean_acc\", ascending=False).reset_index(drop=True)\n",
    "result_fe = result_fe.sort_values(\"mean_acc\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 5 scores: orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>std_acc</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>maxit</th>\n",
       "      <th>l</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8854</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8854</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8852</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.003870</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_acc   std_acc    a     b    maxit     l  step\n",
       "0    0.8855  0.001812  1.1  0.90  96000.0  10.0   1.0\n",
       "1    0.8854  0.001866  1.1  0.90  48000.0  10.0   1.0\n",
       "2    0.8854  0.003248  1.1  0.01  24000.0  10.0   1.0\n",
       "3    0.8852  0.003280  0.9  0.01  24000.0  10.0   1.0\n",
       "4    0.8849  0.003870  1.1  0.10  24000.0  10.0   1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_org.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 5 scores: data after features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>std_acc</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>maxit</th>\n",
       "      <th>l</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8948</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8946</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8944</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_acc   std_acc     a     b    maxit      l  step\n",
       "0    0.8948  0.002138  0.01  0.01  96000.0  10.00   1.0\n",
       "1    0.8946  0.002180  0.01  0.10  96000.0  10.00   1.0\n",
       "2    0.8944  0.001991  0.01  0.01  96000.0   1.00   1.0\n",
       "3    0.8943  0.001994  0.01  0.01  96000.0   0.10   1.0\n",
       "4    0.8943  0.001994  0.01  0.01  96000.0   0.01   1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From c-v results I choose sets of parameters which refer to the best averages of accuracy. Both of them have acceptably low standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on test sets\n",
    "Best models for two data approaches will be scored on test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model for orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_org = result_org.iloc[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a            1.1\n",
       "b            0.9\n",
       "maxit    96000.0\n",
       "l           10.0\n",
       "step         1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_org = Model(a=par_org[0], b=par_org[1], maxit=par_org[2], l=par_org[3], step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model for data after features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_fe = result_fe.iloc[0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a            0.01\n",
       "b            0.01\n",
       "maxit    96000.00\n",
       "l           10.00\n",
       "step         1.00\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fe = Model(a=par_fe[0], b=par_fe[1], maxit=par_fe[2], l=par_fe[3], step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainig best models\n",
    "model_org.fit(X_train, y_train)\n",
    "model_fe.fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating predictions for test sets\n",
    "pred_org = model_org.predict(X_test)\n",
    "pred_fe = model_fe.predict(X_test_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing accuracy of prediction\n",
    "acc_org = model_org.evaluate(pred_org, y_test)\n",
    "acc_fe = model_fe.evaluate(pred_fe, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of best model for orginal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8925\n"
     ]
    }
   ],
   "source": [
    "print(acc_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of best model for data after features engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8927\n"
     ]
    }
   ],
   "source": [
    "print(acc_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value of accuracy metric for both approaches was about 89%.\\\n",
    "This result is quite good and seems to be stable comparing cross-validation and test set scores.\\\n",
    "I managed to get even higher value of accuracy up to about 91%. However result was unstable. Cross-validation method helped to find much more reliable models which perform well on unseen data.\\\n",
    "Features engineering did not help to achive significantly higher scores on test set. On the other hand, equal results mean that reduction of dimensions (features) is possible for this dataset. This conclusion might be useful for building more complex and computationally expensive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
